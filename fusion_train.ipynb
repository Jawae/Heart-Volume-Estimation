{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, Conv2DLayer, MaxPool2DLayer, DenseLayer, GlobalPoolLayer, Upscale2DLayer\n",
    "from lasagne.layers import ElemwiseSumLayer, NonlinearityLayer, SliceLayer, ConcatLayer, ScaleLayer\n",
    "from lasagne.layers import dropout, batch_norm\n",
    "from lasagne.nonlinearities import rectify, softmax, sigmoid\n",
    "from lasagne.init import GlorotNormal, GlorotUniform, HeUniform, HeNormal\n",
    "from lasagne.objectives import squared_error, categorical_crossentropy, categorical_accuracy, binary_accuracy\n",
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import random\n",
    "import theano\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import benchmark as bm\n",
    "import utee\n",
    "from fusion.fcn1.adapter import adapter as adapter1\n",
    "from fusion.fcn2.adapter import adapter as adapter2\n",
    "from fusion.fcn3.adapter import adapter as adapter3\n",
    "from fusion.fcn4.adapter import adapter as adapter4\n",
    "from fusion.fcn5.adapter import adapter as adapter5\n",
    "from fusion.fcn6.adapter import adapter as adapter6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "n_epoches = 20\n",
    "loss_epoches = []\n",
    "snapshot_root = 'fusion_snapshot'\n",
    "n_train_samples = len(x_data_train)\n",
    "n_val_samples = len(x_data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resuming snapshot from fusion/fcn1/96.npz\n"
     ]
    }
   ],
   "source": [
    "# all adapters\n",
    "adapters = []\n",
    "adapters.append(adapter1((48, 48), 'fusion/fcn1/96.npz'))\n",
    "# adapters.append(adapter2((48, 48), 'fusion/fcn2/92.npz'))\n",
    "# adapters.append(adapter3((48, 48), 'fusion/fcn3/52.npz'))\n",
    "# adapters.append(adapter4((48, 48), 'fusion/fcn4/470.npz'))\n",
    "# adapters.append(adapter5((48, 48), 'fusion/fcn5/280.npz'))\n",
    "# adapters.append(adapter6((48, 48), 'fusion/fcn6/114.npz'))\n",
    "\n",
    "# input tensor\n",
    "pred = T.tensor4('pred')\n",
    "location = T.vector('location')\n",
    "resolution = T.matrix('resolution')\n",
    "target_volume = T.fscalar('volume')\n",
    "\n",
    "\n",
    "# fusion layers\n",
    "l_in = InputLayer(shape=(None, len(adapters), fixed_size[0], fixed_size[1]), input_var = pred)\n",
    "mid1 = batch_norm(Conv2DLayer(l_in, num_filters=32, filter_size=(3, 3), nonlinearity=rectify, W=HeNormal()))\n",
    "mid2 = Conv2DLayer(mid1, num_filters=1, filter_size=(1, 1), W=HeNormal())\n",
    "l_out = GlobalPoolLayer(mid2)\n",
    "\n",
    "# area, 1d vector\n",
    "train_area = lasagne.layers.get_output(l_out).flatten()\n",
    "val_area = lasagne.layers.get_output(l_out, deterministic=True).flatten()\n",
    "\n",
    "# predict volume, 0d scalar\n",
    "train_pred_volume = utee.build_volume2(train_area, location, resolution, fixed_size) \n",
    "val_pred_volume = utee.build_volume2(val_area, location, resolution, fixed_size)\n",
    "\n",
    "# loss, 0d scalar\n",
    "train_loss = T.abs_(train_pred_volume - target_volume).mean() / 600.\n",
    "val_loss = T.abs_(val_pred_volume - target_volume).mean() / 600.\n",
    "\n",
    "# params\n",
    "params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(train_loss, params, learning_rate=lr, momentum=0.9)\n",
    "\n",
    "train_fn = theano.function(\n",
    "    [pred, location, resolution, target_volume],\n",
    "    train_loss,\n",
    "    updates = updates\n",
    ")\n",
    "val_fn = theano.function(\n",
    "    [pred, location, resolution, target_volume],\n",
    "    val_loss\n",
    ")\n",
    "test_fn = theano.function(\n",
    "    [pred, location, resolution],\n",
    "    [val_area, val_pred_volume]\n",
    ")\n",
    "\n",
    "area_fn = theano.function(\n",
    "    [pred],\n",
    "    val_area\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and validating precedure begin\n",
      "epoch 0/2 begin\n",
      ".validating, 200 samples to go\n",
      ".validating loss: 0.0390633158386\n",
      "snapshot to fusion_snapshot/0.npz\n",
      "epoch 1/2 begin\n",
      ".training, 800 samples to go\n",
      ".training loss: 0.0224609132856\n",
      ".validating, 200 samples to go\n",
      ".validating loss: 0.0384977124631\n",
      "snapshot to fusion_snapshot/1.npz\n",
      "epoch 2/2 begin\n",
      ".training, 800 samples to go\n",
      ".training loss: 0.0219819135964\n",
      ".validating, 200 samples to go\n",
      ".validating loss: 0.038855869323\n",
      "snapshot to fusion_snapshot/2.npz\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Training and validating precedure begin\")\n",
    "for cur_epoch in range(n_epoches+1):\n",
    "    print(\"epoch {}/{} begin\".format(cur_epoch, n_epoches))\n",
    "    \n",
    "    if cur_epoch > 0:\n",
    "        print(\".training, {} samples to go\".format(n_train_samples))\n",
    "        losses_data_train = []\n",
    "        for j in range(n_train_samples):\n",
    "            x_e = x_data_train[j].astype('float32')\n",
    "            preds = []\n",
    "            for adapter in adapters:\n",
    "                preds.append(adapter.convert(x_e))\n",
    "            pred_e = np.concatenate(preds, axis=1)\n",
    "            location_e = location_data_train[j].astype('float32')\n",
    "            resolution_e = resolution_data_train[j].astype('float32')\n",
    "            volume_e = volume_data_train[j].astype('float32')\n",
    "            loss_data_train = train_fn(pred_e, location_e, resolution_e, volume_e)\n",
    "            losses_data_train.append(loss_data_train)\n",
    "        print(\".training loss: {}\".format(np.mean(loss_data_train)))\n",
    "    \n",
    "    print(\".validating, {} samples to go\".format(n_val_samples))\n",
    "    losses_data_val = []\n",
    "    for i in range(n_val_samples):\n",
    "        volume_min_e = volume_data_val[2*i].astype('float32')\n",
    "        volume_max_e = volume_data_val[2*i+1].astype('float32')\n",
    "        pred_volumes_data = []\n",
    "        for j in range(len(x_data_val[i])):\n",
    "            x_e = x_data_val[i][j].astype('float32')\n",
    "            preds = []\n",
    "            for adapter in adapters:\n",
    "                preds.append(adapter.convert(x_e))\n",
    "            pred_e = np.concatenate(preds, axis=1)\n",
    "            location_e = location_data_val[i][j].astype('float32')\n",
    "            resolution_e = resolution_data_val[i][j].astype('float32')\n",
    "            _, pred_volume_data = test_fn(pred_e, location_e, resolution_e)\n",
    "            pred_volumes_data.append(pred_volume_data)\n",
    "        volume_min_pred = np.min(pred_volumes_data)\n",
    "        volume_max_pred = np.max(pred_volumes_data)\n",
    "        loss_min_data = np.abs(volume_min_pred - volume_min_e) / 600\n",
    "        loss_max_data = np.abs(volume_max_pred - volume_max_e) / 600\n",
    "        loss_data_val = (loss_min_data + loss_max_data) / 2.0\n",
    "    print(\".validating loss: {}\".format(np.mean(losses_data_val)))\n",
    "    \n",
    "    loss_epoches.append(np.mean(losses_data_val))\n",
    "    \n",
    "    if np.isnan(np.mean(losses_data_train)) or np.isnan(np.mean(loss_data_val)):\n",
    "        print(\".detect nan, break and stop\")\n",
    "        break\n",
    "        \n",
    "    cur_snapshot_path = os.path.join(snapshot_root, str(cur_epoch) + '.npz')\n",
    "    print(\"snapshot to {}\".format(cur_snapshot_path))\n",
    "    np.savez(cur_snapshot_path, *lasagne.layers.get_all_param_values(l_out))\n",
    "print(\"Training done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'OUT_VALIDATE_DATA_PATH': u'sdf', u'OUT_TRAIN_DATA_PATH': u'df', u'IN_VALIDATE_DATA_PATH': u'sdf', 'FUSION_SNAPSHOT': 'fusion_snapshot/1.npz', u'IN_TRAIN_DATA_PATH': u'sdf'}\n"
     ]
    }
   ],
   "source": [
    "idx = np.argmin(loss_epoches)\n",
    "with open('./SETTINGS.json', 'r') as f:    \n",
    "    data = json.load(f)\n",
    "print(\"add info {} to SETTINGS.json\".format(snapshot_root, str(idx) +'.npz'))\n",
    "data['FUSION_SNAPSHOT'] = os.path.join()\n",
    "with open('./SETTINGS2.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------predicting---------------------\n",
      "total #samples: 1000, loss: 0.0109496666119\n"
     ]
    }
   ],
   "source": [
    "# print(\"-----------------predicting---------------------\")\n",
    "# losses_data = []\n",
    "# n_samples = int(len(x_data) * 0)\n",
    "# for j in range(len(x_data))[n_samples:]:\n",
    "#     x_e = x_data[j].astype('float32')\n",
    "#     preds = []\n",
    "#     for adapter in adapters:\n",
    "#         preds.append(adapter.convert(x_e))\n",
    "#     pred_e = np.concatenate(preds, axis=1)\n",
    "#     location_e = location_data[j].astype('float32')\n",
    "#     resolution_e = resolution_data[j].astype('float32')\n",
    "#     volume_e = volume_data[j].astype('float32')\n",
    "#     loss_data = fusion_val_fn(pred_e, location_e, resolution_e, volume_e)\n",
    "#     losses_data.append(loss_data)\n",
    "# print(\"total #samples: {}, loss: {}\".format(len(x_data), np.mean(losses_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bm.fusion_submit(adapters, fusion_area_fn, fusion_test_fn, fixed_size, 'submit.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
