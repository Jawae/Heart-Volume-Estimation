{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 3007)\n",
      "/usr/local/lib/python2.7/dist-packages/theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from lasagne.layers import InputLayer, Conv2DLayer, MaxPool2DLayer, DenseLayer, GlobalPoolLayer, Upscale2DLayer\n",
    "from lasagne.layers import ElemwiseSumLayer, NonlinearityLayer, SliceLayer, ConcatLayer, ScaleLayer\n",
    "from lasagne.layers import dropout, batch_norm\n",
    "from lasagne.nonlinearities import rectify, softmax, sigmoid\n",
    "from lasagne.init import GlorotNormal, GlorotUniform, HeUniform, HeNormal\n",
    "from lasagne.objectives import squared_error, categorical_crossentropy, categorical_accuracy, binary_accuracy\n",
    "import lasagne\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "import random\n",
    "import theano\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import benchmark as bm\n",
    "\n",
    "from fcn1.adapter import adapter as adapter1\n",
    "from fcn2.adapter import adapter as adapter2\n",
    "from fcn3.adapter import adapter as adapter3\n",
    "from fcn4.adapter import adapter as adapter4\n",
    "from fcn5.adapter import adapter as adapter5\n",
    "from fcn6.adapter import adapter as adapter6\n",
    "\n",
    "import matplotlib\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_volume2(area, location, resolution, fixed_size):\n",
    "    def _step(idx_, prior_result_, area_, location_, resolution_):\n",
    "        area1 = area_[idx_] * T.prod(resolution_[idx_])\n",
    "        area2 = area_[idx_ + 1] * T.prod(resolution_[idx_ + 1])\n",
    "        h = location_[idx_ + 1] - location_[idx_]\n",
    "        volume = (area1 + area2 )* np.prod(fixed_size).astype('float32') * h / 2.0 / 1000\n",
    "        return prior_result_ + volume\n",
    "\n",
    "    predict_V_list, _ = theano.scan(fn=_step,\n",
    "                              outputs_info = np.array([0.]).astype('float32'),\n",
    "                              sequences = T.arange(1000),\n",
    "                              non_sequences = [area, location, resolution],\n",
    "                              n_steps = location.shape[0] - 1)\n",
    "    predict_V = predict_V_list[-1]\n",
    "    return predict_V[0]\n",
    "\n",
    "def build_volume3(area, location, resolution, fixed_size):\n",
    "    def _step(idx_, prior_result_, area_, location_, resolution_):\n",
    "        area1 = area_[idx_] * T.prod(resolution_[idx_])\n",
    "        area2 = area_[idx_ + 1] * T.prod(resolution_[idx_ + 1])\n",
    "        h = location_[idx_ + 1] - location_[idx_]\n",
    "        volume = (area1 + area2 + T.sqrt(area1 * area2))* np.prod(fixed_size).astype('float32') * h / 3.0 / 1000\n",
    "        return prior_result_ + volume\n",
    "\n",
    "    predict_V_list, _ = theano.scan(fn=_step,\n",
    "                              outputs_info = np.array([0.]).astype('float32'),\n",
    "                              sequences = T.arange(1000),\n",
    "                              non_sequences = [area, location, resolution],\n",
    "                              n_steps = location.shape[0] - 1)\n",
    "    predict_V = predict_V_list[-1]\n",
    "    return predict_V[0]\n",
    "\n",
    "def stage3_load_single_record(file_path, fixed_size):\n",
    "    data = np.load(file_path).item()\n",
    "    patch_list = data['patchStack']\n",
    "    location_list = np.array(data['SliceLocation'])\n",
    "    resolution = np.array(data['PixelSpacing'])\n",
    "    resized_resolution_list = []\n",
    "    resized_patch_list = []\n",
    "    for patch in patch_list:\n",
    "        resized_resolution_list.append(\n",
    "            (resolution[0] / fixed_size[0] * patch.shape[0], resolution[1] / fixed_size[1] * patch.shape[1]))\n",
    "        resized_patch_list.append(cv2.resize(patch, fixed_size))\n",
    "    \n",
    "    resized_patch_list = np.array(resized_patch_list, dtype='float32')[:, None, :, :]\n",
    "    location_list = np.array(location_list, dtype='float32')\n",
    "    resized_resolution_list = np.array(resized_resolution_list, dtype='float32')\n",
    "    return resized_patch_list, location_list, resized_resolution_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fixed_size = (48, 48)\n",
    "# read train and val volume data\n",
    "volume_csv_path = '../clean/stage3/train.csv'\n",
    "volume_csv = pd.read_csv(volume_csv_path)\n",
    "volume_data = np.array(volume_csv.iloc[:, 1:3])\n",
    "rows = volume_data.shape[0]\n",
    "volume_data = volume_data.flatten().astype('float32')\n",
    "\n",
    "# read train and val patch, location and resolution\n",
    "root_dir = '../clean/stage3/'\n",
    "min_root_dir = os.path.join(root_dir, 'min')\n",
    "max_root_dir = os.path.join(root_dir, 'max')\n",
    "x_data = []\n",
    "location_data = []\n",
    "resolution_data = []\n",
    "for i in range(1, rows + 1):\n",
    "    min_full_path = os.path.join(min_root_dir, str(i) + '.npy')\n",
    "    max_full_path = os.path.join(max_root_dir, str(i) + '.npy')\n",
    "    paths = [min_full_path, max_full_path]\n",
    "    for path in paths:\n",
    "        x_data_single, location_data_single, resolution_data_single = stage3_load_single_record(path, fixed_size)\n",
    "        x_data.append(x_data_single)\n",
    "        location_data.append(location_data_single)\n",
    "        resolution_data.append(resolution_data_single)\n",
    "assert len(volume_data) == len(x_data)\n",
    "assert len(x_data) == len(location_data)\n",
    "assert len(location_data) == len(resolution_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resuming snapshot from fcn6/114.npz\n"
     ]
    }
   ],
   "source": [
    "location = T.vector('location')\n",
    "resolution = T.matrix('resolution')\n",
    "target_volume = T.fscalar('volume')\n",
    "\n",
    "# all adapters\n",
    "adapters = []\n",
    "adapters.append(adapter1((48, 48), 'fcn1/96.npz'))\n",
    "adapters.append(adapter2((48, 48), 'fcn2/92.npz'))\n",
    "adapters.append(adapter3((48, 48), 'fcn3/52.npz'))\n",
    "adapters.append(adapter4((48, 48), 'fcn4/470.npz'))\n",
    "adapters.append(adapter5((48, 48), 'fcn5/280.npz'))\n",
    "adapters.append(adapter6((48, 48), 'fcn6/114.npz'))\n",
    "\n",
    "# input tensor\n",
    "pred = T.tensor4('pred')\n",
    "area = T.mean(pred, axis=[1, 2, 3])\n",
    "pred_volume = build_volume3(area, location, resolution, fixed_size) \n",
    "loss = T.abs_(pred_volume - target_volume).mean() / 600.\n",
    "\n",
    "fusion_val_fn = theano.function(\n",
    "    [pred, location, resolution, target_volume],\n",
    "    loss\n",
    ")\n",
    "fusion_test_fn = theano.function(\n",
    "    [pred, location, resolution],\n",
    "    [area, pred_volume]\n",
    ")\n",
    "fusion_area_fn = theano.function(\n",
    "    [pred],\n",
    "    area\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------predicting---------------------\n",
      "total #samples: 1000, loss: 0.0109496666119\n"
     ]
    }
   ],
   "source": [
    "# print(\"-----------------predicting---------------------\")\n",
    "# losses_data = []\n",
    "# n_samples = int(len(x_data) * 0)\n",
    "# for j in range(len(x_data))[n_samples:]:\n",
    "#     x_e = x_data[j].astype('float32')\n",
    "#     preds = []\n",
    "#     for adapter in adapters:\n",
    "#         preds.append(adapter.convert(x_e))\n",
    "#     pred_e = np.concatenate(preds, axis=1)\n",
    "#     location_e = location_data[j].astype('float32')\n",
    "#     resolution_e = resolution_data[j].astype('float32')\n",
    "#     volume_e = volume_data[j].astype('float32')\n",
    "#     loss_data = fusion_val_fn(pred_e, location_e, resolution_e, volume_e)\n",
    "#     losses_data.append(loss_data)\n",
    "# print(\"total #samples: {}, loss: {}\".format(len(x_data), np.mean(losses_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bm.fusion_submit(adapters, fusion_area_fn, fusion_test_fn, fixed_size, 'submit.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
